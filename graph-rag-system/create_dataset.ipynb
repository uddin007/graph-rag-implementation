{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed4958d9-40ef-436c-bfaf-74a16cc097db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Install dependencies (if not already installed)\n",
    "%pip install networkx sentence-transformers scikit-learn mlflow faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46d5a2e-f46e-4ec3-a1de-46b45cd0a1ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Import required libraries\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e42ec5b1-af00-496c-946c-20538eee546b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765771474409}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "item_names = [\n",
    "    \"Choco Bliss\", \"Nutty Crunch\", \"Caramel Dream\", \"Minty Fresh\", \"Berry Burst\",\n",
    "    \"Peanut Delight\", \"Crispy Joy\", \"Fudge Fantasy\", \"Toffee Twist\", \"Coconut Charm\",\n",
    "    \"Almond Supreme\", \"Hazel Heaven\", \"Marshmallow Magic\", \"Cookie Craze\", \"Sugar Rush\",\n",
    "    \"Golden Nugget\", \"Jelly Gem\", \"Cocoa Swirl\", \"Vanilla Velvet\", \"Maple Munch\",\n",
    "    \"Cherry Chew\", \"Orange Zest\", \"Lemon Drop\", \"Gummy Glow\", \"Rainbow Ribbons\",\n",
    "    \"S'mores Sensation\", \"Mocha Melt\", \"Cinnamon Swirl\", \"Pecan Pleasure\", \"Honey Hug\",\n",
    "    \"Strawberry Sizzle\", \"Banana Bonanza\", \"Apple Aroma\", \"Grape Gala\", \"Bubble Bliss\",\n",
    "    \"Espresso Edge\", \"Salted Caramel\", \"Truffle Treat\", \"Pumpkin Pop\", \"Raspberry Ripple\",\n",
    "    \"Mango Magic\", \"Pistachio Punch\", \"Cranberry Crunch\", \"Apricot Adventure\", \"Plum Passion\",\n",
    "    \"Tropical Tango\", \"Lime Lush\", \"Blueberry Bash\", \"Butterscotch Burst\", \"Walnut Whirl\"\n",
    "]\n",
    "\n",
    "descriptions = [\n",
    "    \"Rich chocolate with creamy filling\", \"Crunchy nuts in smooth chocolate\", \"Soft caramel center\",\n",
    "    \"Refreshing mint flavor\", \"Burst of berry goodness\", \"Peanut butter blend\", \"Crispy wafer layers\",\n",
    "    \"Decadent fudge treat\", \"Classic toffee twist\", \"Sweet coconut flakes\", \"Premium almonds inside\",\n",
    "    \"Hazelnut cream filling\", \"Fluffy marshmallow center\", \"Cookie bits in chocolate\", \"Extra sweet sensation\",\n",
    "    \"Golden caramel nuggets\", \"Jelly fruit center\", \"Swirled cocoa delight\", \"Smooth vanilla cream\",\n",
    "    \"Maple infused chocolate\", \"Chewy cherry pieces\", \"Zesty orange flavor\", \"Tangy lemon drop\",\n",
    "    \"Glowing gummy candies\", \"Colorful candy ribbons\", \"S'mores inspired bar\", \"Mocha coffee blend\",\n",
    "    \"Cinnamon spice swirl\", \"Pecan nut crunch\", \"Honey sweetened chocolate\", \"Strawberry infused bar\",\n",
    "    \"Banana flavored treat\", \"Apple flavored candy\", \"Grape jelly center\", \"Bubblegum flavored chocolate\",\n",
    "    \"Espresso infused bar\", \"Salted caramel layer\", \"Rich truffle filling\", \"Pumpkin spice pop\",\n",
    "    \"Raspberry ripple center\", \"Mango flavored delight\", \"Pistachio nut blend\", \"Cranberry crunch bar\",\n",
    "    \"Apricot jam filling\", \"Plum flavored chocolate\", \"Tropical fruit mix\", \"Lime zest infusion\",\n",
    "    \"Blueberry burst center\", \"Butterscotch cream\", \"Walnut swirl filling\"\n",
    "]\n",
    "\n",
    "data = [\n",
    "    (\n",
    "        \"p-\" + str(i+1),\n",
    "        item_names[i],\n",
    "        descriptions[i]\n",
    "    )\n",
    "    for i in range(50)\n",
    "]\n",
    "\n",
    "item_details_df = spark.createDataFrame(\n",
    "    data,\n",
    "    schema=[\"item_id\", \"item_name\", \"item_description\"]\n",
    ")\n",
    "\n",
    "display(item_details_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ae2ab76-55a8-407b-b853-75ad7515e083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "us_cities = [\n",
    "    \"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\", \"San Antonio\", \"San Diego\", \"Dallas\", \"San Jose\",\n",
    "    \"Austin\", \"Jacksonville\", \"Fort Worth\", \"Columbus\", \"Charlotte\", \"San Francisco\", \"Indianapolis\", \"Seattle\", \"Denver\", \"Washington\",\n",
    "    \"Boston\", \"El Paso\", \"Nashville\", \"Detroit\", \"Oklahoma City\", \"Portland\", \"Las Vegas\", \"Memphis\", \"Louisville\", \"Baltimore\",\n",
    "    \"Milwaukee\", \"Albuquerque\", \"Tucson\", \"Fresno\", \"Sacramento\", \"Kansas City\", \"Mesa\", \"Atlanta\", \"Omaha\", \"Colorado Springs\",\n",
    "    \"Raleigh\", \"Miami\", \"Long Beach\", \"Virginia Beach\", \"Oakland\", \"Minneapolis\", \"Tulsa\", \"Arlington\", \"Tampa\", \"New Orleans\"\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"location_id\", StringType(), False),\n",
    "    StructField(\"location_name\", StringType(), True),\n",
    "    StructField(\"location_description\", StringType(), True)\n",
    "])\n",
    "\n",
    "random.shuffle(us_cities)\n",
    "data = [\n",
    "    (\"l-\" + str(i+1), us_cities[i], f\"Store located in {us_cities[i]}, USA\") for i in range(50)\n",
    "]\n",
    "\n",
    "store_location_df = spark.createDataFrame(\n",
    "    data,\n",
    "    schema=schema\n",
    ")\n",
    "display(store_location_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b99fd409-1de0-4192-8b4d-7bbec1d7442f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "\n",
    "customer_data = []\n",
    "for i in range(200):\n",
    "    customer_id = f\"c-{i+1}\"\n",
    "    name = fake.unique.name()\n",
    "    email = fake.unique.email()\n",
    "    zip_code = fake.zipcode_in_state(state_abbr='NY')  # US zip code, can randomize state if desired\n",
    "    customer_data.append((customer_id, name, email, zip_code))\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"customer_email\", StringType(), True),\n",
    "    StructField(\"customer_zip_code\", StringType(), True)\n",
    "])\n",
    "\n",
    "customer_df = spark.createDataFrame(customer_data, schema=customer_schema)\n",
    "display(customer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "033dbd70-09dd-45e3-84be-675edd958765",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766112057605}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_data = []\n",
    "for _ in range(5000):\n",
    "    item_id = random.choice(item_ids)\n",
    "    location_id = random.choice(location_ids)\n",
    "    customer_id = random.choice(customer_ids)\n",
    "    sale_date = random_date()\n",
    "    units_sold = random.randint(1, 20)\n",
    "    unit_price = random.uniform(1.0, 10.0)\n",
    "    total_sales_value = round(units_sold * unit_price, 2)\n",
    "    sales_data.append(\n",
    "        (item_id, location_id, customer_id, sale_date, units_sold, total_sales_value)\n",
    "    )\n",
    "\n",
    "items_sales_df = spark.createDataFrame(\n",
    "    sales_data,\n",
    "    schema=[\n",
    "        \"item_id\",\n",
    "        \"location_id\",\n",
    "        \"customer_id\",\n",
    "        \"sale_date\",\n",
    "        \"units_sold\",\n",
    "        \"total_sales_value\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(items_sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d30b12-adba-4754-a656-4e75df0091a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Save to Unity Catalog\n",
    "catalog = \"accenture\"\n",
    "schema = \"sales_analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e97c63e1-ffbb-404c-bd33-a1fa3f807105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "\n",
    "item_details_df.write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.item_details\")\n",
    "store_location_df.write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.store_location\")\n",
    "customer_df.write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.customer_details\")\n",
    "items_sales_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{catalog}.{schema}.items_sales\")\n",
    "\n",
    "print(\"‚úì Tables saved to Unity Catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81827e83-833e-49dd-b0a8-2b7cd5c12f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Configure Graph RAG\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "@dataclass\n",
    "class GraphRAGConfig:\n",
    "    catalog: str\n",
    "    schema: str\n",
    "    fact_table: str\n",
    "    dimension_tables: List[str]\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\"\n",
    "    top_k_nodes: int = 5\n",
    "    max_hops: int = 2\n",
    "    fk_mappings: Optional[Dict] = None\n",
    "\n",
    "config = GraphRAGConfig(\n",
    "    catalog=catalog,\n",
    "    schema=schema,\n",
    "    fact_table=\"items_sales\",\n",
    "    dimension_tables=[\"item_details\", \"store_location\"],\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    top_k_nodes=5,\n",
    "    max_hops=2,\n",
    "    fk_mappings={\n",
    "        \"items_sales\": {\n",
    "            \"item_id\": \"item_details\",\n",
    "            \"location_id\": \"store_location\",\n",
    "            \"customer_id\": \"customer_details\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úì Configuration created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66278635-e7be-47eb-ae71-459fe66eff3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Build Knowledge Graph\n",
    "class KnowledgeGraphBuilder:\n",
    "    def __init__(self, config: GraphRAGConfig):\n",
    "        self.config = config\n",
    "        self.graph = nx.MultiDiGraph()\n",
    "        \n",
    "    def build_from_tables(self):\n",
    "        \"\"\"Build graph from dimension and fact tables\"\"\"\n",
    "        \n",
    "        # Load dimension tables as nodes\n",
    "        for dim_table in self.config.dimension_tables:\n",
    "            table_name = f\"{self.config.catalog}.{self.config.schema}.{dim_table}\"\n",
    "            df = spark.table(table_name).toPandas()\n",
    "            \n",
    "            # First column is assumed to be the ID\n",
    "            id_col = df.columns[0]\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                node_id = f\"{dim_table}_{row[id_col]}\"\n",
    "                attributes = {col: str(row[col]) for col in df.columns}\n",
    "                attributes['_table'] = dim_table\n",
    "                self.graph.add_node(node_id, **attributes)\n",
    "        \n",
    "        print(f\"‚úì Added {self.graph.number_of_nodes()} nodes from dimension tables\")\n",
    "        \n",
    "        # Load fact table as edges\n",
    "        fact_table_name = f\"{self.config.catalog}.{self.config.schema}.{self.config.fact_table}\"\n",
    "        fact_df = spark.table(fact_table_name).toPandas()\n",
    "        \n",
    "        # Get FK mappings\n",
    "        fk_mappings = self.config.fk_mappings.get(self.config.fact_table, {})\n",
    "        \n",
    "        edges_added = 0\n",
    "        for _, row in fact_df.iterrows():\n",
    "            edge_attrs = {col: row[col] for col in fact_df.columns}\n",
    "            \n",
    "            # Create edges between dimension entities\n",
    "            source_fks = []\n",
    "            for fk_col, dim_table in fk_mappings.items():\n",
    "                if fk_col in row:\n",
    "                    source_fks.append((fk_col, dim_table, row[fk_col]))\n",
    "            \n",
    "            # Create edges between all pairs\n",
    "            for i, (fk1, dim1, val1) in enumerate(source_fks):\n",
    "                for fk2, dim2, val2 in source_fks[i+1:]:\n",
    "                    node1 = f\"{dim1}_{val1}\"\n",
    "                    node2 = f\"{dim2}_{val2}\"\n",
    "                    \n",
    "                    if self.graph.has_node(node1) and self.graph.has_node(node2):\n",
    "                        self.graph.add_edge(node1, node2, **edge_attrs)\n",
    "                        edges_added += 1\n",
    "        \n",
    "        print(f\"‚úì Added {edges_added} edges from fact table\")\n",
    "        return self.graph\n",
    "\n",
    "# Build the graph\n",
    "builder = KnowledgeGraphBuilder(config)\n",
    "graph = builder.build_from_tables()\n",
    "\n",
    "print(f\"\\nüìä Graph Statistics:\")\n",
    "print(f\"   Total Nodes: {graph.number_of_nodes()}\")\n",
    "print(f\"   Total Edges: {graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9090743e-3f9f-40a9-a7e3-834bb15a6bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 7: Generate embeddings\n",
    "print(\"\\nüîÑ Generating embeddings...\")\n",
    "model = SentenceTransformer(config.embedding_model)\n",
    "\n",
    "node_texts = {}\n",
    "node_embeddings = {}\n",
    "\n",
    "for node_id, attrs in graph.nodes(data=True):\n",
    "    # Create text representation of node\n",
    "    text_parts = [f\"{k}: {v}\" for k, v in attrs.items() if not k.startswith('_')]\n",
    "    node_text = \", \".join(text_parts)\n",
    "    node_texts[node_id] = node_text\n",
    "    \n",
    "# Generate embeddings in batch\n",
    "all_texts = list(node_texts.values())\n",
    "all_node_ids = list(node_texts.keys())\n",
    "embeddings = model.encode(all_texts, show_progress_bar=True)\n",
    "\n",
    "for node_id, embedding in zip(all_node_ids, embeddings):\n",
    "    node_embeddings[node_id] = embedding\n",
    "\n",
    "print(f\"‚úì Generated embeddings for {len(node_embeddings)} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7c2be8b-298f-4211-b011-38e99d60f86e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 8: Test query\n",
    "def query_graph(question: str, top_k: int = 5, max_hops: int = 2):\n",
    "    \"\"\"Query the graph with natural language\"\"\"\n",
    "    \n",
    "    # Generate question embedding\n",
    "    question_embedding = model.encode([question])[0]\n",
    "    \n",
    "    # Find similar nodes\n",
    "    similarities = {}\n",
    "    for node_id, node_emb in node_embeddings.items():\n",
    "        sim = cosine_similarity([question_embedding], [node_emb])[0][0]\n",
    "        similarities[node_id] = sim\n",
    "    \n",
    "    # Get top K nodes\n",
    "    top_nodes = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    \n",
    "    print(f\"\\nüîç Top {top_k} matching nodes:\")\n",
    "    for node_id, score in top_nodes:\n",
    "        print(f\"   {node_id}: {score:.3f} - {node_texts[node_id][:80]}...\")\n",
    "    \n",
    "    # Traverse graph\n",
    "    subgraph_nodes = set()\n",
    "    for node_id, _ in top_nodes:\n",
    "        subgraph_nodes.add(node_id)\n",
    "        \n",
    "        # Multi-hop traversal\n",
    "        current_nodes = {node_id}\n",
    "        for hop in range(max_hops):\n",
    "            next_nodes = set()\n",
    "            for node in current_nodes:\n",
    "                neighbors = set(graph.neighbors(node))\n",
    "                predecessors = set(graph.predecessors(node))\n",
    "                next_nodes.update(neighbors)\n",
    "                next_nodes.update(predecessors)\n",
    "            subgraph_nodes.update(next_nodes)\n",
    "            current_nodes = next_nodes\n",
    "    \n",
    "    # Extract subgraph\n",
    "    subgraph = graph.subgraph(subgraph_nodes)\n",
    "    \n",
    "    print(f\"\\nüìä Subgraph: {subgraph.number_of_nodes()} nodes, {subgraph.number_of_edges()} edges\")\n",
    "    \n",
    "    # Generate answer\n",
    "    answer_parts = []\n",
    "    for node in subgraph.nodes():\n",
    "        answer_parts.append(f\"- {node}: {node_texts[node]}\")\n",
    "    \n",
    "    return \"\\n\".join(answer_parts[:10])  # Limit to top 10 for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ebe2fc8-d30e-474e-8b25-60843a805c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test queries\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING QUERIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Query 1\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"\\nüìù Query: Show top 3 sales items by total sales value\")\n",
    "result = query_graph(\"Show high sales items\")\n",
    "print(f\"\\nüí° Answer:\\n{result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Graph RAG implementation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b828554b-b9f6-439e-99c3-02c0be852fc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class EnhancedGraphRAG:\n",
    "    def __init__(self, graph, node_embeddings, node_texts, config, spark):\n",
    "        self.graph = graph\n",
    "        self.node_embeddings = node_embeddings\n",
    "        self.node_texts = node_texts\n",
    "        self.config = config\n",
    "        self.spark = spark\n",
    "        self.model = SentenceTransformer(config.embedding_model)\n",
    "        \n",
    "    def classify_query_intent(self, question: str) -> Dict:\n",
    "        \"\"\"Classify what the user is asking for\"\"\"\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        # Aggregation keywords\n",
    "        agg_keywords = ['top', 'highest', 'most', 'best', 'total', 'sum', \n",
    "                        'average', 'max', 'min', 'count', 'bottom', 'worst']\n",
    "        \n",
    "        # Measure keywords\n",
    "        measure_keywords = {\n",
    "            'sales': 'total_sales_value',\n",
    "            'revenue': 'total_sales_value',\n",
    "            'units': 'units_sold',\n",
    "            'quantity': 'units_sold',\n",
    "            'volume': 'units_sold'\n",
    "        }\n",
    "        \n",
    "        # Check for aggregation intent\n",
    "        has_aggregation = any(keyword in question_lower for keyword in agg_keywords)\n",
    "        \n",
    "        # Detect which measure\n",
    "        measure_col = None\n",
    "        for keyword, col in measure_keywords.items():\n",
    "            if keyword in question_lower:\n",
    "                measure_col = col\n",
    "                break\n",
    "        \n",
    "        # Detect entity type\n",
    "        entity_type = None\n",
    "        if any(word in question_lower for word in ['item', 'product', 'goods']):\n",
    "            entity_type = 'item_details'\n",
    "        elif any(word in question_lower for word in ['location', 'store', 'place']):\n",
    "            entity_type = 'store_location'\n",
    "        \n",
    "        # Detect number requested\n",
    "        limit = 5  # default\n",
    "        number_match = re.search(r'top (\\d+)|(\\d+) (top|best|highest)', question_lower)\n",
    "        if number_match:\n",
    "            limit = int(number_match.group(1) or number_match.group(2))\n",
    "        \n",
    "        return {\n",
    "            'is_aggregation': has_aggregation,\n",
    "            'measure': measure_col,\n",
    "            'entity_type': entity_type,\n",
    "            'limit': limit,\n",
    "            'intent': 'aggregation' if has_aggregation else 'semantic_search'\n",
    "        }\n",
    "    \n",
    "    def execute_aggregation_query(self, intent: Dict) -> str:\n",
    "        \"\"\"Execute aggregation-based query using Spark SQL\"\"\"\n",
    "        \n",
    "        measure = intent['measure'] or 'total_sales_value'\n",
    "        entity_type = intent['entity_type'] or 'item_details'\n",
    "        limit = intent['limit']\n",
    "        \n",
    "        # Map entity type to table and join column\n",
    "        if entity_type == 'item_details':\n",
    "            entity_col = 'item_id'\n",
    "            entity_table = f\"{self.config.catalog}.{self.config.schema}.item_details\"\n",
    "            name_col = 'item_name'\n",
    "        else:\n",
    "            entity_col = 'location_id'\n",
    "            entity_table = f\"{self.config.catalog}.{self.config.schema}.store_location\"\n",
    "            name_col = 'location_name'\n",
    "        \n",
    "        # Build SQL query\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            e.{entity_col},\n",
    "            e.{name_col},\n",
    "            SUM(f.{measure}) as total_measure,\n",
    "            COUNT(*) as transaction_count\n",
    "        FROM {self.config.catalog}.{self.config.schema}.{self.config.fact_table} f\n",
    "        JOIN {entity_table} e ON f.{entity_col} = e.{entity_col}\n",
    "        GROUP BY e.{entity_col}, e.{name_col}\n",
    "        ORDER BY total_measure DESC\n",
    "        LIMIT {limit}\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\nüîç Executing aggregation query...\")\n",
    "        print(f\"   Measure: {measure}\")\n",
    "        print(f\"   Entity: {entity_type}\")\n",
    "        print(f\"   Limit: {limit}\")\n",
    "        \n",
    "        # Execute query\n",
    "        result_df = self.spark.sql(query)\n",
    "        results = result_df.collect()\n",
    "        \n",
    "        # Format answer\n",
    "        answer_parts = [f\"\\nTop {limit} {entity_type.replace('_', ' ')} by {measure}:\\n\"]\n",
    "        \n",
    "        for i, row in enumerate(results, 1):\n",
    "            name = row[name_col]\n",
    "            value = row['total_measure']\n",
    "            count = row['transaction_count']\n",
    "            answer_parts.append(\n",
    "                f\"{i}. {name} - ${value:,.2f} ({count} transactions)\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n\".join(answer_parts)\n",
    "    \n",
    "    def execute_semantic_search(self, question: str, top_k: int = 5, max_hops: int = 2) -> str:\n",
    "        \"\"\"Execute semantic similarity search (original method)\"\"\"\n",
    "        \n",
    "        # Generate question embedding\n",
    "        question_embedding = self.model.encode([question])[0]\n",
    "        \n",
    "        # Find similar nodes\n",
    "        similarities = {}\n",
    "        for node_id, node_emb in self.node_embeddings.items():\n",
    "            sim = cosine_similarity([question_embedding], [node_emb])[0][0]\n",
    "            similarities[node_id] = sim\n",
    "        \n",
    "        # Get top K nodes\n",
    "        top_nodes = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        print(f\"\\nüîç Top {top_k} semantically similar nodes:\")\n",
    "        for node_id, score in top_nodes:\n",
    "            print(f\"   {node_id}: {score:.3f}\")\n",
    "        \n",
    "        # Build answer from top nodes\n",
    "        answer_parts = []\n",
    "        for node_id, score in top_nodes:\n",
    "            node_text = self.node_texts[node_id]\n",
    "            answer_parts.append(f\"- {node_text} (similarity: {score:.3f})\")\n",
    "        \n",
    "        return \"\\n\".join(answer_parts)\n",
    "    \n",
    "    def query(self, question: str) -> str:\n",
    "        \"\"\"Main query method with intent classification\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìù Query: {question}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Classify intent\n",
    "        intent = self.classify_query_intent(question)\n",
    "        print(f\"\\nüß† Query Intent: {intent['intent']}\")\n",
    "        print(f\"   Details: {intent}\")\n",
    "        \n",
    "        # Route to appropriate handler\n",
    "        if intent['is_aggregation'] and intent['measure']:\n",
    "            result = self.execute_aggregation_query(intent)\n",
    "        else:\n",
    "            result = self.execute_semantic_search(question, top_k=5, max_hops=2)\n",
    "        \n",
    "        print(f\"\\nüí° Answer:\")\n",
    "        print(result)\n",
    "        print(f\"\\n{'='*80}\\n\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# Initialize the enhanced system\n",
    "enhanced_rag = EnhancedGraphRAG(\n",
    "    graph=graph,\n",
    "    node_embeddings=node_embeddings,\n",
    "    node_texts=node_texts,\n",
    "    config=config,\n",
    "    spark=spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0dac33b-dd00-4346-b8af-242c393f33aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query 1: Aggregation query\n",
    "enhanced_rag.query(\"Show top 5 sales items by total sales value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace1a973-4b7b-426e-90c3-3c018b16feb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT s.item_id, SUM(s.total_sales_value) AS total_sales, i.item_name, i.item_description\n",
    "FROM accenture.sales_analysis.items_sales s\n",
    "JOIN accenture.sales_analysis.item_details i ON s.item_id = i.item_id\n",
    "GROUP BY s.item_id, i.item_name, i.item_description\n",
    "ORDER BY total_sales DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "top_items_df = spark.sql(query)\n",
    "display(top_items_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e296baa-6485-46d2-8669-e1bdc9f207fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query 2: Another aggregation\n",
    "enhanced_rag.query(\"What are the top 5 items by revenue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2447fb1a-6fd5-40b3-a30b-c64e29c2ea39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT s.item_id, SUM(s.total_sales_value) AS total_sales, i.item_name, i.item_description\n",
    "FROM accenture.sales_analysis.items_sales s\n",
    "JOIN accenture.sales_analysis.item_details i ON s.item_id = i.item_id\n",
    "GROUP BY s.item_id, i.item_name, i.item_description\n",
    "ORDER BY total_sales DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "top_items_df = spark.sql(query)\n",
    "display(top_items_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8710fd8-722d-4d82-aaf0-9ef9337d9c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query 3: Location-based aggregation\n",
    "enhanced_rag.query(\"List top 5 store location_name that have highest sales?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9709dfd5-fad1-4094-8e7f-08a6cbb2bc7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "  l.location_name, \n",
    "  SUM(s.total_sales_value) AS total_sales_value, \n",
    "  COUNT(*) AS transaction_count\n",
    "FROM accenture.sales_analysis.items_sales s\n",
    "JOIN accenture.sales_analysis.store_location l \n",
    "  ON s.location_id = l.location_id\n",
    "GROUP BY l.location_name\n",
    "ORDER BY total_sales_value DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "top_locations_df = spark.sql(query)\n",
    "display(top_locations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f041fbf0-a3c6-4e05-895d-b1a454e2b815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query 4: Time-based aggregation\n",
    "enhanced_rag.query(\"List the top 5 items by units_sold in December 2025?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e61861a-6453-4f48-a770-98d3d69c80b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "  i.item_id,\n",
    "  i.item_name,\n",
    "  SUM(s.units_sold) AS total_units_sold,\n",
    "  COUNT(*) AS transaction_count\n",
    "FROM accenture.sales_analysis.items_sales s\n",
    "JOIN accenture.sales_analysis.item_details i ON s.item_id = i.item_id\n",
    "WHERE YEAR(s.sale_date) = 2025 AND MONTH(s.sale_date) = 12\n",
    "GROUP BY i.item_id, i.item_name\n",
    "ORDER BY total_units_sold DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "top_items_units_sold_df = spark.sql(query)\n",
    "display(top_items_units_sold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c247f5fd-5a57-476b-9eeb-f05c78118bce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class EnhancedGraphRAG:\n",
    "    def __init__(self, graph, node_embeddings, node_texts, config, spark):\n",
    "        self.graph = graph\n",
    "        self.node_embeddings = node_embeddings\n",
    "        self.node_texts = node_texts\n",
    "        self.config = config\n",
    "        self.spark = spark\n",
    "        self.model = SentenceTransformer(config.embedding_model)\n",
    "        \n",
    "    def extract_date_filters(self, question: str) -> Dict:\n",
    "        \"\"\"Extract date/time filters from natural language query\"\"\"\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        filters = {\n",
    "            'year': None,\n",
    "            'month': None,\n",
    "            'quarter': None,\n",
    "            'date_range': None\n",
    "        }\n",
    "        \n",
    "        # Extract year (e.g., \"2025\", \"in 2024\")\n",
    "        year_match = re.search(r'\\b(20\\d{2})\\b', question)\n",
    "        if year_match:\n",
    "            filters['year'] = int(year_match.group(1))\n",
    "        \n",
    "        # Extract month by name or number\n",
    "        month_names = {\n",
    "            'january': 1, 'jan': 1,\n",
    "            'february': 2, 'feb': 2,\n",
    "            'march': 3, 'mar': 3,\n",
    "            'april': 4, 'apr': 4,\n",
    "            'may': 5,\n",
    "            'june': 6, 'jun': 6,\n",
    "            'july': 7, 'jul': 7,\n",
    "            'august': 8, 'aug': 8,\n",
    "            'september': 9, 'sep': 9, 'sept': 9,\n",
    "            'october': 10, 'oct': 10,\n",
    "            'november': 11, 'nov': 11,\n",
    "            'december': 12, 'dec': 12\n",
    "        }\n",
    "        \n",
    "        for month_name, month_num in month_names.items():\n",
    "            if month_name in question_lower:\n",
    "                filters['month'] = month_num\n",
    "                break\n",
    "        \n",
    "        # Extract month number (e.g., \"month 12\", \"12/2025\")\n",
    "        if not filters['month']:\n",
    "            month_match = re.search(r'\\b(month\\s+)?(\\d{1,2})[/\\-]', question_lower)\n",
    "            if month_match:\n",
    "                month = int(month_match.group(2))\n",
    "                if 1 <= month <= 12:\n",
    "                    filters['month'] = month\n",
    "        \n",
    "        # Extract quarter (Q1, Q2, Q3, Q4)\n",
    "        quarter_match = re.search(r'\\bq([1-4])\\b', question_lower)\n",
    "        if quarter_match:\n",
    "            filters['quarter'] = int(quarter_match.group(1))\n",
    "        \n",
    "        # Extract relative dates\n",
    "        if 'last month' in question_lower:\n",
    "            current = datetime.now()\n",
    "            filters['month'] = current.month - 1 if current.month > 1 else 12\n",
    "            filters['year'] = current.year if current.month > 1 else current.year - 1\n",
    "        elif 'this month' in question_lower:\n",
    "            current = datetime.now()\n",
    "            filters['month'] = current.month\n",
    "            filters['year'] = current.year\n",
    "        elif 'last year' in question_lower:\n",
    "            filters['year'] = datetime.now().year - 1\n",
    "        elif 'this year' in question_lower:\n",
    "            filters['year'] = datetime.now().year\n",
    "        \n",
    "        return filters\n",
    "    \n",
    "    def classify_query_intent(self, question: str) -> Dict:\n",
    "        \"\"\"Classify what the user is asking for\"\"\"\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        # Aggregation keywords\n",
    "        agg_keywords = ['top', 'highest', 'most', 'best', 'total', 'sum', \n",
    "                        'average', 'max', 'min', 'count', 'bottom', 'worst', 'list']\n",
    "        \n",
    "        # Measure keywords\n",
    "        measure_keywords = {\n",
    "            'sales': 'total_sales_value',\n",
    "            'revenue': 'total_sales_value',\n",
    "            'units': 'units_sold',\n",
    "            'quantity': 'units_sold',\n",
    "            'volume': 'units_sold',\n",
    "            'transactions': 'COUNT(*)'\n",
    "        }\n",
    "        \n",
    "        # Check for aggregation intent\n",
    "        has_aggregation = any(keyword in question_lower for keyword in agg_keywords)\n",
    "        \n",
    "        # Detect which measure\n",
    "        measure_col = None\n",
    "        for keyword, col in measure_keywords.items():\n",
    "            if keyword in question_lower:\n",
    "                measure_col = col\n",
    "                break\n",
    "        \n",
    "        # Default to sales value if aggregation but no measure specified\n",
    "        if has_aggregation and not measure_col:\n",
    "            measure_col = 'total_sales_value'\n",
    "        \n",
    "        # Detect entity type\n",
    "        entity_type = None\n",
    "        if any(word in question_lower for word in ['item', 'product', 'goods']):\n",
    "            entity_type = 'item_details'\n",
    "        elif any(word in question_lower for word in ['location', 'store', 'place', 'shop']):\n",
    "            entity_type = 'store_location'\n",
    "        \n",
    "        # Default to items if not specified\n",
    "        if has_aggregation and not entity_type:\n",
    "            entity_type = 'item_details'\n",
    "        \n",
    "        # Detect number requested\n",
    "        limit = 5  # default\n",
    "        number_match = re.search(r'top (\\d+)|(\\d+) (top|best|highest)', question_lower)\n",
    "        if number_match:\n",
    "            limit = int(number_match.group(1) or number_match.group(2))\n",
    "        \n",
    "        # Extract date filters\n",
    "        date_filters = self.extract_date_filters(question)\n",
    "        \n",
    "        return {\n",
    "            'is_aggregation': has_aggregation,\n",
    "            'measure': measure_col,\n",
    "            'entity_type': entity_type,\n",
    "            'limit': limit,\n",
    "            'date_filters': date_filters,\n",
    "            'intent': 'aggregation' if has_aggregation else 'semantic_search'\n",
    "        }\n",
    "    \n",
    "    def build_date_where_clause(self, date_filters: Dict, table_alias: str = 'f') -> str:\n",
    "        \"\"\"Build SQL WHERE clause for date filters\"\"\"\n",
    "        conditions = []\n",
    "        \n",
    "        if date_filters['year']:\n",
    "            conditions.append(f\"YEAR({table_alias}.sale_date) = {date_filters['year']}\")\n",
    "        \n",
    "        if date_filters['month']:\n",
    "            conditions.append(f\"MONTH({table_alias}.sale_date) = {date_filters['month']}\")\n",
    "        \n",
    "        if date_filters['quarter']:\n",
    "            conditions.append(f\"QUARTER({table_alias}.sale_date) = {date_filters['quarter']}\")\n",
    "        \n",
    "        if date_filters['date_range']:\n",
    "            start, end = date_filters['date_range']\n",
    "            conditions.append(f\"{table_alias}.sale_date BETWEEN '{start}' AND '{end}'\")\n",
    "        \n",
    "        return \" AND \".join(conditions) if conditions else \"1=1\"\n",
    "    \n",
    "    def execute_aggregation_query(self, intent: Dict) -> str:\n",
    "        \"\"\"Execute aggregation-based query using Spark SQL\"\"\"\n",
    "        \n",
    "        measure = intent['measure']\n",
    "        entity_type = intent['entity_type']\n",
    "        limit = intent['limit']\n",
    "        date_filters = intent['date_filters']\n",
    "        \n",
    "        # Map entity type to table and join column\n",
    "        if entity_type == 'item_details':\n",
    "            entity_col = 'item_id'\n",
    "            entity_table = f\"{self.config.catalog}.{self.config.schema}.item_details\"\n",
    "            name_col = 'item_name'\n",
    "        else:\n",
    "            entity_col = 'location_id'\n",
    "            entity_table = f\"{self.config.catalog}.{self.config.schema}.store_location\"\n",
    "            name_col = 'location_name'\n",
    "        \n",
    "        # Build WHERE clause for date filters\n",
    "        where_clause = self.build_date_where_clause(date_filters, 'f')\n",
    "        \n",
    "        # Build aggregation expression\n",
    "        if measure == 'COUNT(*)':\n",
    "            agg_expr = 'COUNT(*)'\n",
    "            measure_label = 'transaction_count'\n",
    "        else:\n",
    "            agg_expr = f'SUM(f.{measure})'\n",
    "            measure_label = 'total_measure'\n",
    "        \n",
    "        # Build SQL query\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            e.{entity_col},\n",
    "            e.{name_col},\n",
    "            {agg_expr} as {measure_label},\n",
    "            COUNT(*) as transaction_count\n",
    "        FROM {self.config.catalog}.{self.config.schema}.{self.config.fact_table} f\n",
    "        JOIN {entity_table} e ON f.{entity_col} = e.{entity_col}\n",
    "        WHERE {where_clause}\n",
    "        GROUP BY e.{entity_col}, e.{name_col}\n",
    "        ORDER BY {measure_label} DESC\n",
    "        LIMIT {limit}\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\nüîç Executing aggregation query...\")\n",
    "        print(f\"   Measure: {measure}\")\n",
    "        print(f\"   Entity: {entity_type}\")\n",
    "        print(f\"   Limit: {limit}\")\n",
    "        if any(date_filters.values()):\n",
    "            print(f\"   Date Filters: {date_filters}\")\n",
    "        print(f\"\\nüìã SQL Query:\")\n",
    "        print(query)\n",
    "        \n",
    "        # Execute query\n",
    "        result_df = self.spark.sql(query)\n",
    "        results = result_df.collect()\n",
    "        \n",
    "        # Format answer\n",
    "        date_desc = \"\"\n",
    "        if date_filters['month'] and date_filters['year']:\n",
    "            month_names = ['', 'January', 'February', 'March', 'April', 'May', 'June',\n",
    "                          'July', 'August', 'September', 'October', 'November', 'December']\n",
    "            date_desc = f\" in {month_names[date_filters['month']]} {date_filters['year']}\"\n",
    "        elif date_filters['year']:\n",
    "            date_desc = f\" in {date_filters['year']}\"\n",
    "        elif date_filters['quarter'] and date_filters['year']:\n",
    "            date_desc = f\" in Q{date_filters['quarter']} {date_filters['year']}\"\n",
    "        \n",
    "        answer_parts = [f\"\\nTop {limit} {entity_type.replace('_', ' ')}{date_desc} by {measure}:\\n\"]\n",
    "        \n",
    "        if not results:\n",
    "            return f\"\\nNo data found for the specified filters{date_desc}.\"\n",
    "        \n",
    "        for i, row in enumerate(results, 1):\n",
    "            name = row[name_col]\n",
    "            value = row[measure_label]\n",
    "            count = row['transaction_count']\n",
    "            \n",
    "            # Format value based on measure type\n",
    "            if measure == 'total_sales_value':\n",
    "                value_str = f\"${value:,.2f}\"\n",
    "            elif measure == 'COUNT(*)':\n",
    "                value_str = f\"{int(value):,} transactions\"\n",
    "            else:\n",
    "                value_str = f\"{value:,.0f} units\"\n",
    "            \n",
    "            answer_parts.append(\n",
    "                f\"{i}. {name} - {value_str} ({count} transactions)\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n\".join(answer_parts)\n",
    "    \n",
    "    def execute_semantic_search(self, question: str, top_k: int = 5, max_hops: int = 2) -> str:\n",
    "        \"\"\"Execute semantic similarity search (original method)\"\"\"\n",
    "        \n",
    "        # Generate question embedding\n",
    "        question_embedding = self.model.encode([question])[0]\n",
    "        \n",
    "        # Find similar nodes\n",
    "        similarities = {}\n",
    "        for node_id, node_emb in self.node_embeddings.items():\n",
    "            sim = cosine_similarity([question_embedding], [node_emb])[0][0]\n",
    "            similarities[node_id] = sim\n",
    "        \n",
    "        # Get top K nodes\n",
    "        top_nodes = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        print(f\"\\nüîç Top {top_k} semantically similar nodes:\")\n",
    "        for node_id, score in top_nodes:\n",
    "            print(f\"   {node_id}: {score:.3f}\")\n",
    "        \n",
    "        # Build answer from top nodes\n",
    "        answer_parts = []\n",
    "        for node_id, score in top_nodes:\n",
    "            node_text = self.node_texts[node_id]\n",
    "            answer_parts.append(f\"- {node_text} (similarity: {score:.3f})\")\n",
    "        \n",
    "        return \"\\n\".join(answer_parts)\n",
    "    \n",
    "    def query(self, question: str) -> str:\n",
    "        \"\"\"Main query method with intent classification\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìù Query: {question}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Classify intent\n",
    "        intent = self.classify_query_intent(question)\n",
    "        print(f\"\\nüß† Query Intent: {intent['intent']}\")\n",
    "        print(f\"   Details: {intent}\")\n",
    "        \n",
    "        # Route to appropriate handler\n",
    "        if intent['is_aggregation'] and intent['measure']:\n",
    "            result = self.execute_aggregation_query(intent)\n",
    "        else:\n",
    "            result = self.execute_semantic_search(question, top_k=5, max_hops=2)\n",
    "        \n",
    "        print(f\"\\nüí° Answer:\")\n",
    "        print(result)\n",
    "        print(f\"\\n{'='*80}\\n\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# Re-initialize with enhanced version\n",
    "enhanced_rag = EnhancedGraphRAG(\n",
    "    graph=graph,\n",
    "    node_embeddings=node_embeddings,\n",
    "    node_texts=node_texts,\n",
    "    config=config,\n",
    "    spark=spark\n",
    ")\n",
    "\n",
    "# Test all your queries\n",
    "print(\"\\nüß™ TESTING TIME-BASED QUERIES\\n\")\n",
    "\n",
    "# Test 1: Month + Year filter\n",
    "enhanced_rag.query(\"List the top 5 items by units_sold in December 2025?\")\n",
    "\n",
    "# Test 2: Just year filter\n",
    "enhanced_rag.query(\"What are the top 3 items by revenue in 2025?\")\n",
    "\n",
    "# Test 3: Quarter filter\n",
    "enhanced_rag.query(\"Show top 5 locations by sales in Q4 2025\")\n",
    "\n",
    "# Test 4: Month name\n",
    "enhanced_rag.query(\"Top selling items in November 2025\")\n",
    "\n",
    "# Test 5: No date filter (all time)\n",
    "enhanced_rag.query(\"What are the top 5 items by revenue?\")\n",
    "\n",
    "# Test 6: Relative date\n",
    "enhanced_rag.query(\"Show top items this year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d3729f4-c18f-4289-91e7-4c89dbec9d75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "  i.item_id,\n",
    "  i.item_name,\n",
    "  SUM(s.units_sold) AS total_units_sold,\n",
    "  COUNT(*) AS transaction_count\n",
    "FROM accenture.sales_analysis.items_sales s\n",
    "JOIN accenture.sales_analysis.item_details i ON s.item_id = i.item_id\n",
    "WHERE YEAR(s.sale_date) = 2025 AND MONTH(s.sale_date) = 12\n",
    "GROUP BY i.item_id, i.item_name\n",
    "ORDER BY total_units_sold DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "top_items_units_sold_df = spark.sql(query)\n",
    "display(top_items_units_sold_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "create_dataset",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
