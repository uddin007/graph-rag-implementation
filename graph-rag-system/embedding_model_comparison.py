# Databricks notebook source
# MAGIC %md
# MAGIC # Embedding Model Comparison for Graph RAG
# MAGIC
# MAGIC This notebook compares different embedding models for your Graph RAG system.
# MAGIC
# MAGIC **Models Tested:**
# MAGIC 1. all-MiniLM-L6-v2 (current - fast, lightweight)
# MAGIC 2. BAAI/bge-m3 (recommended - high quality)
# MAGIC 3. BAAI/bge-base-en-v1.5 (balanced)
# MAGIC 4. BAAI/bge-large-en-v1.5 (highest quality)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Setup

# COMMAND ----------

# MAGIC %pip install networkx sentence-transformers scikit-learn --quiet

# COMMAND ----------

from graph_rag_core import GraphRAGConfig, GraphRAGSystem
from graph_rag_enhancements import enhance_with_all

import time
import pandas as pd

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. View Available Models

# COMMAND ----------

# List all available models with details
GraphRAGConfig.list_models()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Test Different Models

# COMMAND ----------

# Define test queries
test_queries = [
    "Show me chocolate items",
    "Find premium dark chocolate products",
    "Items with rich smooth flavor",
    "Berry flavored products",
    "Sweet treats with nuts"
]

# COMMAND ----------

# MAGIC %md
# MAGIC ### 3.1 Test with all-MiniLM-L6-v2 (Current - Fast)

# COMMAND ----------

print("="*80)
print("TESTING: all-MiniLM-L6-v2 (Current Model)")
print("="*80)

# Build system with MiniLM
config_mini = GraphRAGConfig(
    catalog="accenture",
    schema="sales_analysis",
    fact_table="items_sales",
    dimension_tables=["item_details", "store_location", "customer_details"],
    fk_mappings={
        "items_sales": {
            "item_id": "item_details",
            "location_id": "store_location",
            "customer_id": "customer_details"
        }
    },
    embedding_model="mini"  # Use preset
)

start_time = time.time()
system_mini = GraphRAGSystem(config_mini, spark)
system_mini.build()
enhance_with_all(system_mini)
build_time_mini = time.time() - start_time

print(f"\n‚úÖ Build time: {build_time_mini:.2f} seconds")

# COMMAND ----------

# Test queries with MiniLM
results_mini = {}

for query in test_queries:
    print(f"\n{'='*80}")
    print(f"Query: {query}")
    print(f"{'='*80}")
    
    start_time = time.time()
    answer = system_mini.query(query, verbose=False)
    query_time = time.time() - start_time
    
    results_mini[query] = {
        'answer': answer,
        'time': query_time
    }
    
    print(f"\n‚è±Ô∏è Query time: {query_time:.3f} seconds")
    print(f"\nüí° Answer:\n{answer[:500]}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### 3.2 Test with BGE-M3 (Recommended for Graph RAG)

# COMMAND ----------

print("="*80)
print("TESTING: BAAI/bge-m3 (Recommended for Graph RAG)")
print("="*80)

# Build system with BGE-M3
config_bge = GraphRAGConfig(
    catalog="accenture",
    schema="sales_analysis",
    fact_table="items_sales",
    dimension_tables=["item_details", "store_location", "customer_details"],
    fk_mappings={
        "items_sales": {
            "item_id": "item_details",
            "location_id": "store_location",
            "customer_id": "customer_details"
        }
    },
    embedding_model="bge-m3"  # Use BGE-M3
)

start_time = time.time()
system_bge = GraphRAGSystem(config_bge, spark)
system_bge.build()
enhance_with_all(system_bge)
build_time_bge = time.time() - start_time

print(f"\n‚úÖ Build time: {build_time_bge:.2f} seconds")

# COMMAND ----------

# Test queries with BGE-M3
results_bge = {}

for query in test_queries:
    print(f"\n{'='*80}")
    print(f"Query: {query}")
    print(f"{'='*80}")
    
    start_time = time.time()
    answer = system_bge.query(query, verbose=False)
    query_time = time.time() - start_time
    
    results_bge[query] = {
        'answer': answer,
        'time': query_time
    }
    
    print(f"\n‚è±Ô∏è Query time: {query_time:.3f} seconds")
    print(f"\nüí° Answer:\n{answer[:500]}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Side-by-Side Comparison

# COMMAND ----------

# Compare results side by side
print("="*80)
print("SIDE-BY-SIDE COMPARISON")
print("="*80)

for query in test_queries:
    print(f"\n{'='*80}")
    print(f"Query: {query}")
    print(f"{'='*80}")
    
    print("\nüìå MiniLM Results:")
    print(results_mini[query]['answer'][:300])
    print(f"‚è±Ô∏è Time: {results_mini[query]['time']:.3f}s")
    
    print("\nüìå BGE-M3 Results:")
    print(results_bge[query]['answer'][:300])
    print(f"‚è±Ô∏è Time: {results_bge[query]['time']:.3f}s")
    
    print(f"\n‚öñÔ∏è Time difference: {results_bge[query]['time'] - results_mini[query]['time']:.3f}s")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Performance Summary

# COMMAND ----------

# Create performance comparison table
comparison_data = {
    'Metric': [
        'Build Time',
        'Avg Query Time',
        'Embedding Dimensions',
        'Model Size',
        'Best For'
    ],
    'all-MiniLM-L6-v2': [
        f"{build_time_mini:.2f}s",
        f"{sum(r['time'] for r in results_mini.values()) / len(results_mini):.3f}s",
        "384",
        "~200MB",
        "Speed, small graphs"
    ],
    'BAAI/bge-m3': [
        f"{build_time_bge:.2f}s",
        f"{sum(r['time'] for r in results_bge.values()) / len(results_bge):.3f}s",
        "1024",
        "~2.3GB",
        "Quality, large graphs"
    ]
}

comparison_df = pd.DataFrame(comparison_data)
display(comparison_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Quality Assessment

# COMMAND ----------

# Manually assess quality for each query
print("="*80)
print("QUALITY ASSESSMENT")
print("="*80)
print("\nFor each query, compare the relevance of results:")
print("- Are chocolate items ranked higher in BGE-M3?")
print("- Does BGE-M3 better understand nuanced queries?")
print("- Are semantic relationships clearer?")
print("\nManually review the results above to determine which model")
print("provides better semantic understanding for your use case.")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Recommendation

# COMMAND ----------

print("="*80)
print("RECOMMENDATION")
print("="*80)

print(f"""
Based on your testing results:

üìä BUILD TIME:
   ‚Ä¢ MiniLM: {build_time_mini:.2f}s
   ‚Ä¢ BGE-M3: {build_time_bge:.2f}s
   ‚Ä¢ Difference: {build_time_bge - build_time_mini:.2f}s ({(build_time_bge/build_time_mini - 1)*100:.1f}% slower)

‚è±Ô∏è QUERY SPEED:
   ‚Ä¢ MiniLM: {sum(r['time'] for r in results_mini.values()) / len(results_mini):.3f}s avg
   ‚Ä¢ BGE-M3: {sum(r['time'] for r in results_bge.values()) / len(results_bge):.3f}s avg
   ‚Ä¢ After build, both are fast enough (<1 sec)

üéØ RECOMMENDATION:

For DEVELOPMENT (iterating, testing):
   ‚Üí Use MiniLM (faster builds)
   ‚Üí config = GraphRAGConfig(..., embedding_model="mini")

For PRODUCTION (deployed, quality matters):
   ‚Üí Use BGE-M3 (better semantic understanding)
   ‚Üí config = GraphRAGConfig(..., embedding_model="bge-m3")

For LARGE GRAPHS (>100K nodes):
   ‚Üí Use BGE-M3 (scales better, more precise)

For MULTI-LINGUAL:
   ‚Üí Use BGE-M3 (supports 139 languages)

üí° HYBRID APPROACH:
   ‚Ä¢ Develop with MiniLM (fast iteration)
   ‚Ä¢ Deploy with BGE-M3 (production quality)
   ‚Ä¢ Build system once, query many times (build time amortized)
""")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. How to Switch Models in Production

# COMMAND ----------

print("""
SWITCHING MODELS IN YOUR CODE:

Option 1: Use Preset Names (Recommended)
=========================================
config = GraphRAGConfig(
    catalog="accenture",
    schema="sales_analysis",
    fact_table="items_sales",
    dimension_tables=["item_details", "store_location", "customer_details"],
    fk_mappings={...},
    embedding_model="bge-m3"  # ‚Üê Just change this!
)

Available presets:
- "mini" ‚Üí all-MiniLM-L6-v2 (fast)
- "bge-m3" ‚Üí BAAI/bge-m3 (recommended)
- "bge-base" ‚Üí BAAI/bge-base-en-v1.5 (balanced)
- "bge-large" ‚Üí BAAI/bge-large-en-v1.5 (highest quality)
- "e5-small" ‚Üí intfloat/e5-small-v2 (alternative fast)


Option 2: Use Full Model Name
==============================
config = GraphRAGConfig(
    ...,
    embedding_model="BAAI/bge-m3"  # ‚Üê Full HuggingFace model name
)


Option 3: Environment Variable
===============================
import os

model = os.getenv("EMBEDDING_MODEL", "bge-m3")  # Default to BGE-M3

config = GraphRAGConfig(
    ...,
    embedding_model=model
)


Option 4: Configuration File (Best for Production)
===================================================
# config.yaml
embedding:
  model: "bge-m3"
  top_k: 5

# Load in code
import yaml
with open("config.yaml") as f:
    cfg = yaml.safe_load(f)

config = GraphRAGConfig(
    ...,
    embedding_model=cfg['embedding']['model']
)
""")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 9. Save Both Models (Optional)

# COMMAND ----------

# If you want to save both models to MLflow
from graph_rag_enhancements import save_graph_rag_model

# Save MiniLM version
print("Saving MiniLM model...")
save_graph_rag_model(system_mini, "sales_graph_rag_mini")

# Save BGE-M3 version
print("\nSaving BGE-M3 model...")
save_graph_rag_model(system_bge, "sales_graph_rag_bge_m3")

print("\n‚úÖ Both models saved!")
print("   ‚Ä¢ sales_graph_rag_mini (fast)")
print("   ‚Ä¢ sales_graph_rag_bge_m3 (high quality)")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Summary
# MAGIC
# MAGIC **Key Findings:**
# MAGIC - BGE-M3 is 5-10x slower to build but provides better quality
# MAGIC - Query time difference is negligible (<1 sec for both)
# MAGIC - BGE-M3 recommended for production Graph RAG applications
# MAGIC - MiniLM good for development and fast iteration
# MAGIC
# MAGIC **Next Steps:**
# MAGIC 1. Review quality differences in results above
# MAGIC 2. Choose model based on your priorities (speed vs quality)
# MAGIC 3. Update your production config with chosen model
# MAGIC 4. Build once, query many times (amortize build cost)
